{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geomloss import SamplesLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch\n",
    "import madgrad\n",
    "import torch.autograd as auto\n",
    "sys.path.append(\"./set_transformer\")\n",
    "from modules import ISAB, PMA, SAB\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import RobustScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "def build_dataset():\n",
    "    data_gpaw = pd.read_pickle(\"data/gpaw_forces_dataframe.pickle\")\n",
    "    data_vasp = pd.read_pickle(\"data/VASP_MoS2_defects.pickle\")\n",
    "    assert (data_vasp.structures.iloc[0].lattice._matrix == data_gpaw.structure.iloc[0].cell.array).all()\n",
    "    \n",
    "    positions = np.concatenate([i.positions[np.newaxis, :, :] for i in data_gpaw.structure] +\n",
    "                               [i.cart_coords[np.newaxis, :, :] for i in data_vasp.structures],\n",
    "                               axis=0).astype(np.float32)\n",
    "    #positions -= positions.mean(axis=(0,1), keepdims=True)\n",
    "    #positions /= positions.std(axis=(0,1), keepdims=True)\n",
    "    \n",
    "    energies = np.concatenate([data_gpaw.energy.values, data_vasp.energy.values]).astype(np.float32)\n",
    "#     energies -= energies.mean()\n",
    "    energies = energies.reshape(-1, 1)\n",
    "    transformer = RobustScaler().fit(energies)\n",
    "\n",
    "    energies = transformer.transform(energies)\n",
    "\n",
    "\n",
    "    forces = np.concatenate([f[np.newaxis, :, :] for f in data_gpaw.forces] +\n",
    "                            [np.zeros((len(data_vasp), positions.shape[1], positions.shape[2]), dtype=np.float32)],\n",
    "                            axis=0).astype(np.float32)\n",
    "    \n",
    "    types = np.concatenate([i.get_atomic_numbers()[np.newaxis, :] for i in data_gpaw.structure] + \n",
    "                           [np.array(i.atomic_numbers)[np.newaxis, :] for i in data_vasp.structures],\n",
    "                           axis=0).astype(np.int32)\n",
    "    return list(map(np.array,\n",
    "                    train_test_split(positions, types, energies, forces, test_size=0.25, random_state=1421))),\\\n",
    "           data_gpaw.structure.iloc[0].cell.array.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fixup_initialization(args):\n",
    "    temp_state_dic = {}\n",
    "    en_layers = args.encoder_layers\n",
    "    de_layers = args.decoder_layers\n",
    "\n",
    "    if args.Tfixup:\n",
    "        for name, param in self.named_parameters():\n",
    "            if name in [\"fc1.weight\",\n",
    "                        \"fc2.weight\",\n",
    "                        \"self_attn.out_proj.weight\",\n",
    "                        ]:\n",
    "                temp_state_dic[name] = (0.67 * (en_layers) ** (- 1. / 4.)) * param\n",
    "            elif name in [\"self_attn.v_proj.weight\",]:\n",
    "                temp_state_dic[name] = (0.67 * (en_layers) ** (- 1. / 4.)) * (param * (2**0.5))\n",
    "\n",
    "    for name in self.state_dict():\n",
    "        if name not in temp_state_dic:\n",
    "            temp_state_dic[name] = self.state_dict()[name]\n",
    "    self.load_state_dict(temp_state_dic)\n",
    "    \n",
    "\n",
    "# temp_state_dict = embed_tokens.state_dict()\n",
    "# temp_state_dict[\"weight\"] = (9 * args.encoder_layers) ** (- 1 / 4) * temp_state_dict[\"weight\"]\n",
    "# embed_tokens.load_state_dict(temp_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SetTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim_input=4,\n",
    "        num_outputs=1,\n",
    "        dim_output=1,\n",
    "        num_inds=32,\n",
    "        dim_hidden=128,\n",
    "        num_heads=16,\n",
    "        ln=False,\n",
    "    ):\n",
    "        super(SetTransformer, self).__init__()\n",
    "        self.enc = nn.Sequential(\n",
    "            SAB(dim_input, dim_hidden, num_heads, ln=ln),\n",
    "            SAB(dim_hidden, dim_hidden, num_heads, ln=ln),\n",
    "            ISAB(dim_hidden, dim_hidden, num_heads, num_inds, ln=ln),\n",
    "#             ISAB(dim_hidden, dim_hidden, num_heads, num_inds, ln=ln),\n",
    "#             ISAB(dim_hidden, dim_hidden, num_heads, num_inds, ln=ln),\n",
    "#             ISAB(dim_hidden, dim_hidden, num_heads, num_inds, ln=ln),\n",
    "#             ISAB(dim_hidden, dim_hidden, num_heads, num_inds, ln=ln),\n",
    "        )\n",
    "        self.dec = nn.Sequential(\n",
    "            PMA(dim_hidden, num_heads, num_outputs, ln=ln),\n",
    "            nn.Linear(dim_hidden, dim_hidden),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(dim_hidden, dim_hidden),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(dim_hidden, dim_output),\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.dec(self.enc(X)).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positions.shape = (616, 105, 3)\n",
      "<E> = -0.7444171905517578\n"
     ]
    }
   ],
   "source": [
    "(positions, test_positions, \\\n",
    "types, test_types, \\\n",
    "energies, test_energies, \\\n",
    "forces, test_forces), lattice = build_dataset()\n",
    "species = (types==types[0,0]).astype(np.float32)\n",
    "energy_mean = np.mean(energies)\n",
    "print('positions.shape = {}'.format(positions.shape))\n",
    "print('<E> = {}'.format(energy_mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_t = torch.tensor(np.concatenate([positions, np.expand_dims(species, -1)], axis=2), requires_grad=True).cuda()\n",
    "energies_t = torch.tensor(energies).cuda()\n",
    "forces_t = torch.tensor(forces).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=['enc.2.I', 'enc.2.mab0.fc_q.weight', 'enc.2.mab0.fc_q.bias', 'enc.2.mab0.fc_k.weight', 'enc.2.mab0.fc_k.bias', 'enc.2.mab0.fc_v.weight', 'enc.2.mab0.fc_v.bias', 'enc.2.mab0.fc_o.weight', 'enc.2.mab0.fc_o.bias', 'enc.2.mab1.fc_q.weight', 'enc.2.mab1.fc_q.bias', 'enc.2.mab1.fc_k.weight', 'enc.2.mab1.fc_k.bias', 'enc.2.mab1.fc_v.weight', 'enc.2.mab1.fc_v.bias', 'enc.2.mab1.fc_o.weight', 'enc.2.mab1.fc_o.bias'], unexpected_keys=['enc.3.I', 'enc.3.mab0.fc_q.weight', 'enc.3.mab0.fc_q.bias', 'enc.3.mab0.fc_k.weight', 'enc.3.mab0.fc_k.bias', 'enc.3.mab0.fc_v.weight', 'enc.3.mab0.fc_v.bias', 'enc.3.mab0.fc_o.weight', 'enc.3.mab0.fc_o.bias', 'enc.3.mab1.fc_q.weight', 'enc.3.mab1.fc_q.bias', 'enc.3.mab1.fc_k.weight', 'enc.3.mab1.fc_k.bias', 'enc.3.mab1.fc_v.weight', 'enc.3.mab1.fc_v.bias', 'enc.3.mab1.fc_o.weight', 'enc.3.mab1.fc_o.bias', 'enc.4.I', 'enc.4.mab0.fc_q.weight', 'enc.4.mab0.fc_q.bias', 'enc.4.mab0.fc_k.weight', 'enc.4.mab0.fc_k.bias', 'enc.4.mab0.fc_v.weight', 'enc.4.mab0.fc_v.bias', 'enc.4.mab0.fc_o.weight', 'enc.4.mab0.fc_o.bias', 'enc.4.mab1.fc_q.weight', 'enc.4.mab1.fc_q.bias', 'enc.4.mab1.fc_k.weight', 'enc.4.mab1.fc_k.bias', 'enc.4.mab1.fc_v.weight', 'enc.4.mab1.fc_v.bias', 'enc.4.mab1.fc_o.weight', 'enc.4.mab1.fc_o.bias', 'enc.2.mab.fc_q.weight', 'enc.2.mab.fc_q.bias', 'enc.2.mab.fc_k.weight', 'enc.2.mab.fc_k.bias', 'enc.2.mab.fc_v.weight', 'enc.2.mab.fc_v.bias', 'enc.2.mab.fc_o.weight', 'enc.2.mab.fc_o.bias'])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SetTransformer(dim_hidden=256, num_heads=4, num_inds=16).cuda()\n",
    "model.load_state_dict(torch.load('data/0.263_ev.pth'), strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4437  LR: 0.000007  loss: 0.18036767840385437 Energy RMSE: 0.407 eV         Force RMSE: 0.381 eV\r"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1000, gamma=0.9)\n",
    "\n",
    "\n",
    "force_coefficient = 0.1\n",
    "losses_en = []\n",
    "losses_f = []\n",
    "\n",
    "\n",
    "model.train()\n",
    "for iteration in range(10000):\n",
    "    preds = model(inputs_t)\n",
    "    energies_t = energies_t.reshape(-1)\n",
    "    mse_en = F.mse_loss(preds, energies_t)\n",
    "#     l1 = l1(preds, energies_t)\n",
    "    pred_forces = - auto.grad(preds.sum(), inputs_t, retain_graph=True,\n",
    "                        create_graph=True)[0]\n",
    "    mse_f = F.mse_loss(pred_forces[..., :3], forces_t)\n",
    "\n",
    "    loss = mse_en + force_coefficient * mse_f\n",
    "    \n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    losses_en.append(mse_en.item())\n",
    "    losses_f.append(mse_f.item())\n",
    "\n",
    "    \n",
    "    print(f\"{iteration}  LR: {scheduler.get_last_lr()[0]:.6f}  loss: {loss.cpu().detach().item()} Energy RMSE: {np.sqrt(losses_en[-1]):.3f} eV \\\n",
    "        Force RMSE: {np.sqrt(losses_f[-1]):.3f} eV\", end='\\r')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save model\n",
    "# torch.save(model.state_dict(), 'data/0.263_ev.pth')\n",
    "# # Just incase...\n",
    "# torch.save(optimizer.state_dict(), 'data/optim_0.263_ev.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_np = preds.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(energies, preds_np)\n",
    "ax.set_ylabel(\"Normed predicted total energy, eV\")\n",
    "ax.set_xlabel(\"Normed DFT total energy, eV\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(energies, '.')\n",
    "plt.plot(preds_np, 'r.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing\n",
    "(positions, test_positions, \\\n",
    "types, test_types, \\\n",
    "energies, test_energies, \\\n",
    "forces, test_forces), lattice = build_dataset()\n",
    "species = (test_types==test_types[0,0]).astype(np.float32)\n",
    "energy_mean_test = np.mean(test_energies)\n",
    "print('positions.shape = {}'.format(test_positions.shape))\n",
    "print('<E> = {}'.format(energy_mean_test))\n",
    "\n",
    "\n",
    "inputs_test = torch.tensor(np.concatenate([test_positions, np.expand_dims(species, -1)], axis=2), requires_grad=True).cuda()\n",
    "energies_test = torch.tensor(test_energies)\n",
    "preds_test = model(inputs_test)\n",
    "\n",
    "print(f'Test rmse: {torch.nn.MSELoss()(energies_test.reshape(-1), preds_test.cpu()).sqrt().item()} ev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_test_ = preds_test.cpu().detach().numpy()\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(energies_test, preds_test_)\n",
    "ax.set_ylabel(\"Normed predicted total energy, eV\")\n",
    "ax.set_xlabel(\"Normed DFT total energy, eV\");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(energies_test, '.')\n",
    "plt.plot(preds_test_, 'r.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_positions_ = torch.tensor(test_positions, requires_grad=True).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "force = - auto.grad(preds_test.sum(), test_positions_)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(((test_forces - force_)**2).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(test_forces[..., 0], force_[..., 0], 'b.', alpha=0.5)\n",
    "ax.plot(test_forces[..., 0], force_[..., 0], 'g.', alpha=0.5)\n",
    "ax.plot(test_forces[..., 0], force_[..., 0], 'r.', alpha=0.5)\n",
    "\n",
    "ax.set_ylabel(\"Force\")\n",
    "ax.set_xlabel(\"Predicted force\");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameter search..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_t = torch.Tensor(np.concatenate([positions, np.expand_dims(species, -1)], axis=2)).cuda()\n",
    "energies_t = torch.Tensor(energies).cuda()\n",
    "\n",
    "def objective(trial):\n",
    "    lr = trial.suggest_uniform('lr', 1e-6, 1e-3)\n",
    "    \n",
    "    model = SetTransformer(dim_hidden=256, num_heads=16).cuda()\n",
    "    # optimizer = torch.optim.Adam(model.parameters(), lr=1e-7)\n",
    "    optimizer = madgrad.MADGRAD(model.parameters(), lr=lr)\n",
    "#     scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=1e-6,\n",
    "#                                                 anneal_strategy='linear', div_factor=100,\n",
    "#                                                 steps_per_epoch=1,\n",
    "#                                                 epochs=5000)\n",
    "\n",
    "    losses = []\n",
    "    mse = torch.nn.MSELoss()#SamplesLoss(\"laplacian\", blur=0.1)\n",
    "    l1 = torch.nn.L1Loss()\n",
    "    model.train()\n",
    "    for iteration in range(10):\n",
    "        preds = model(inputs_t)\n",
    "    #     loss = criterion(preds.unsqueeze(0).view(1, -1, 1), energies_t.unsqueeze(0))\n",
    "        global energies_t\n",
    "        energies_t = energies_t.reshape(-1)\n",
    "        l2 = mse(preds, energies_t)\n",
    "        loss = l2 + l1(preds, energies_t)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    #     scheduler.step()\n",
    "        losses.append(l2.item())\n",
    "        print(f\"{iteration}  Train RMSE {np.sqrt(losses[-1]):.3f} eV\", end='\\r')\n",
    "    return np.sqrt(losses[-1])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pruner: optuna.pruners.BasePruner = (optuna.pruners.MedianPruner())\n",
    "    \n",
    "logging.basicConfig(filename='search.log', level=logging.INFO)\n",
    "\n",
    "def print_best_callback(study, trial):\n",
    "    with open('search.log', 'a+') as file:\n",
    "        file.writelines(f\"Best value: {study.best_value}, Best params: {study.best_trial.params}\") \n",
    "    print(f\"Best value: {study.best_value}, Best params: {study.best_trial.params}\")\n",
    "\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\", pruner=pruner)\n",
    "study.optimize(objective, n_trials=300, n_jobs=1, callbacks=[print_best_callback])\n",
    "\n",
    "print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"  Value: {}\".format(trial.value))\n",
    "\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor(float('inf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study.optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
